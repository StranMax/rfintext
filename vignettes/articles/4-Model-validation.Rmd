---
title: "4 Model validation"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE, warning=FALSE}
library(rfintext)
# devtools::install_github("StranMax/rfinstats")
library(rfinstats)
library(dplyr)
library(tidyr)
library(tidytext)
library(xgboost)
library(caret)
library(topicmodels)
library(quanteda)
library(forcats)
library(purrr)
library(future)
library(furrr)
plan(multisession, workers = availableCores(logical = FALSE) - 1)
```

```{r, include=FALSE}
quanteda_options(print_dfm_max_ndoc = 20, print_dfm_max_nfeat = 20)
```

```{r, message=FALSE, R.options=list(width=10000, pillar.print_max = 20, pillar.print_min = 20)}
y <- rfinstats::taantuvat |>
  filter(kunta %in% unique(aspol$kunta)) |>
  mutate(luokka = factor(if_else(suht_muutos_2010_2022 > 0, "kasvava", "taantuva")))
y
```

```{r}
dtm <- aspol |>
  preprocess_corpus(kunta) |>
  corpus_to_dtm(kunta, LEMMA)
```

```{r}
optimal_k <- c(5, 7, 10, 15, 21)  # Visually selected based on mean topic coherence (see article 3)
ptm <- proc.time()
lda_models <- tibble(K = optimal_k) |>
  mutate(
    # LDA models
    topic_model = future_map(  
      K, ~LDA(convert(dtm, to = "tm"), k = ., control = list(seed = 1234)),  # LDA does not support quanteda dfm-format
      .options = furrr_options(seed = NULL)
    ),
    # Theta matrix (gamma)
    theta = map(
      topic_model, \(x) {
        tidy(x, matrix = "gamma") |> 
          rename(theta = gamma) |>
          filter(document %in% y$kunta) |>
          cast_dfm(document = document, term = topic, value = theta)
      }
    )
  )
proc.time() - ptm
lda_models
```

Divide data set to training and test parts:

```{r}
trainidx <- createDataPartition(y$suht_muutos_2010_2022, p = .7,
                                list = FALSE, 
                                times = 1)
```

```{r}
lda_models <- lda_models |>
  mutate(
    train = map(theta, \(x) {
      dfm_subset(x, docname_ %in% y$kunta[trainidx])
    }),
    test = map(theta, \(x) {
      dfm_subset(x, docname_ %in% y$kunta[-trainidx])
    })
  )
lda_models
```


```{r}
lda_models <- lda_models |>
  mutate(
    xgb_train = map(train, \(train) {
      xgb.DMatrix(data = train, label = as.integer(y$luokka[trainidx]) - 1)  # Class levels to integers (starting from 0)
    }),
    xgb_test = map(test, \(test) {
      xgb.DMatrix(data = test, label = as.integer(y$luokka[-trainidx]) - 1)  # Class levels to integers (starting from 0)
    })
  )
lda_models
```

Define set of parameters for hyper parameter tuning:

```{r, R.options=list(width=10000, pillar.print_max = 20, pillar.print_min = 20)}
gs <- tidyr::expand_grid(
  booster = "gbtree",
  eta = seq(0.01, 0.1, by = 0.2),
  max_depth = seq(2, 7, by = 1),
  gamma = seq(0, 4, by = 2),
  subsample = seq(0.5, 1, by = 0.25),
  colsample_bylevel = seq(0.5, 1, by = 0.25),
  nrounds = seq(5, 55, by = 25),
  # objective = "reg:squarederror",
  objective = "binary:logistic",
  num_parallel_tree = 2,
)
gs
```

Add combination of lda-models to hyper parameter set:

```{r}
xgb_models <- expand_grid(select(lda_models, K), gs) |> 
  left_join(select(lda_models, K, data = xgb_train, test_data = xgb_test))
xgb_models 
```

Now we are ready to train models:

```{r}
ptm <- proc.time()
xgb_models <- xgb_models |>
  mutate(model = pmap(select(xgb_models, -K, -test_data), xgb.train))
proc.time() - ptm
xgb_models
```

Calculate errors for all models:

```{r}
xgb_models <- xgb_models |>
  mutate(
    error = map2_dbl(model, test_data, \(model, test_data) {
      label = xgboost::getinfo(test_data, "label")
      pred <- stats::predict(model, test_data)
      err <- as.numeric(sum(as.integer(pred > 0.5) != label))/length(label)
      err
    })
  )
xgb_models
```

Smallest errors appear to be `r min(xgb_models$error)`

```{r}
xgb_models |> arrange(error)
```


```{r, fig.width=14, fig.height=4}
xgb_models |>
  ggplot(aes(x = error)) +
  geom_boxplot() +
  coord_flip() +
  facet_wrap(~K, ncol = 5)
```


```{r}
xgb_models |>
  ggplot(aes(x = error)) +
  geom_boxplot() +
  coord_flip() +
  facet_wrap(~max_depth, ncol = 6)
```

Feature importance:  

```{r}
xgb_models <- xgb_models |>
  mutate(
    feat_importance = map2(data, model, \(data, model)  {
      xgb.importance(feature_names = colnames(data),
                     model = model)
    })
  )
xgb_models
```

```{r, fig.width=14, fig.height=7}
xgb_models |>
  select(K, feat_importance) |>
  unnest(feat_importance) |>
  mutate(Feature = factor(as.integer(Feature))) |>
  ggplot(aes(x = Feature, y = Gain)) +
  geom_boxplot() +
  labs(title = "Feature importance", x = "Feature (topic)") +
  facet_wrap(~K, scales = "free_x")
```


