---
title: "1 Data preprocessing"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Examples of pre processing text data with `dplyr` and `stringr` in data frame. 
Many of the pre process steps introduced here are possible to carry out after 
conversion to document-term-matrix using `quanteda` package presented in the 
next articles.

Aim is to filter out uninteresting, and potentially false terms. 
Background for pre processing text data:  
* Ristilä A. & K. Elo (2023). Observing political and societal changes in Finnish 
parliamentary speech data, 1980–2010, with topic modelling. 
*Parliaments, Estates and Representation*, 43:2, 149–176.  
* Matthew J. D. & A. Spirling (2018). Text Preprocessing For Unsupervised Learning:
Why It Matters, When It Misleads, And What To Do About It. *Political Analysis*
vol. 26:168–189. 

```{r setup, message=FALSE, warning=FALSE}
library(rfintext)
library(stringr)
library(dplyr)
```

**Common preprocessing steps include:**  
1. Select only nouns verbs and adjectives   
2. Remove words which contain numbers  
3. Filter out foreign terms  
4. Drop terms which appear in less than 0.5% of documents (at least 5 docs here) 
5. Drop terms which appear in more than 99% of documents (at most 65 docs here)  
6. Additionally we drop very short terms (under 2 character in original FORM and 
under 3 characters in base LEMMA)  

Original terms total `r nrow(aspol)`

```{r}
aspol |> count(LEMMA, sort = TRUE)
```

Let's take a closer look what we are throwing away while carrying out our pre 
processing pipeline.

Filtering nouns, verbs and adjectives drop following terms:

```{r}
aspol |> filter(!UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>  # Note negation operator `!` here
  count(LEMMA, sort = TRUE)
```

Now we filter nouns, verbs and adjectives.Which of those words contain numbers 
(in their original FORM at least).

```{r}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(str_detect(FORM, "[0-9]+")) |> 
  count(LEMMA, sort = TRUE)
```

> Note. FORM `m2` get´s LEMMA `menrer#mammen`!?  

Nouns, verbs or adjectives with no numbers, but marked as a foreign term. 
These are mostly nonsense here. We have already gotten rid of real foreign terms 
by filtering nouns, verbs and adjectives, since foreign terms have UPOSTAG == "X".

```{r}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(!str_detect(FORM, "[0-9]+")) |>  # Note negation operator `!`
  filter(str_detect(FEATS, "Foreign=Yes")) |>  
  count(LEMMA, sort = TRUE)
```


Very common terms in terms of document frequency:

> NOTE! Document frequency is easy to get by following dplyr functions:  
> `aspol |> distinct(kunta, LEMMA) |> count(LEMMA, name = "doc_freq")`


```{r}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(!str_detect(FORM, "[0-9]+")) |>
  filter(!str_detect(FEATS, "Foreign=Yes")) |>
  left_join(
    aspol |> distinct(kunta, LEMMA) |> count(LEMMA, name = "df")  # Adds document-frequency to table with dplyr functions
  ) |>
  filter(df > 65) |>
  count(LEMMA, sort = TRUE)
```


Very uncommon terms in terms of document frequency: 

```{r}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(!str_detect(FORM, "[0-9]+")) |>
  filter(!str_detect(FEATS, "Foreign=Yes")) |>
   left_join(
    aspol |> distinct(kunta, LEMMA) |> count(LEMMA, name = "df")
  ) |>
  filter(df < 5) |>
  count(LEMMA, sort = TRUE)
```


Nouns, verbs or adjectives with numbers removed, foreign (or nonsense) removed, 
but very short terms still present:

```{r}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(!str_detect(FORM, "[0-9]+")) |>
  filter(!str_detect(FEATS, "Foreign=Yes")) |>
  left_join(
    aspol |> distinct(kunta, LEMMA) |> count(LEMMA, name = "df")
  ) |>
  filter(df >= 5, df <= 65) |>
  filter(nchar(FORM) < 3, nchar(LEMMA) < 4) |>
  count(LEMMA, sort = TRUE)
```


Complete pre processing pipeline:  

```{r, R.options = list(width = 10000, pillar.print_max = 20, pillar.print_min = 20)}
aspol |>
  filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ")) |>
  filter(!str_detect(FORM, "[0-9]+")) |>
  filter(!str_detect(FEATS, "Foreign=Yes")) |>
  left_join(
    aspol |> distinct(kunta, LEMMA) |> count(LEMMA, name = "df")
  ) |>
  filter(df >= 5, df <= 65) |>
  filter(nchar(FORM) >= 3, nchar(LEMMA) >= 4)
```

Complete preprocessing pipeline as a single function. This is exactly the same 
as above but easier to type. 

```{r, R.options = list(width = 10000, pillar.print_max = 20, pillar.print_min = 20)}
aspol |> preprocess_corpus(doc = kunta)
```

> NOTE! Results depend on pre processing steps, details and even order they have 
been conducted. 
