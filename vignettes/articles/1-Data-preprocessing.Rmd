---
title: "1 Data preprocessing"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Aim is to filter out uninteresting, and potentially false terms.
This data preprocessing article leans much on two articles:  
* Ristilä A. & K. Elo (2023). Observing political and societal changes in Finnish 
parliamentary speech data, 1980–2010, with topic modelling. 
*Parliaments, Estates and Representation*, 43:2, 149–176.  
* Matthew J. D. & A. Spirling (2018). Text Preprocessing For Unsupervised Learning:
Why It Matters, When It Misleads, And What To Do About It. *Political Analysis*
vol. 26:168–189.

```{r setup, message=FALSE, warning=FALSE}
library(rfintext)
library(stringr)
library(dplyr)
```

**Common preprocessing steps include:**  
1. Select only nouns verbs and adjectives  
2. Remove words which contain numbers  
3. Filter out foreign terms  
4. Drop terms which appear in less than 0.5% of documents (at least 5 docs here) 
5. Drop terms which appear in more than 99% of documents (at most 65 docs here)

Original terms total `r nrow(aspol)`

```{r}
aspol |> count(LEMMA, sort = TRUE)
```

```{r}
aspol |> filter_upostag(c("NOUN", "VERB", "ADJ")) |> count(LEMMA, sort = TRUE)
```

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(FORM) |>
  nrow()
```

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  nrow()
```

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  remove_short_term(len = 4) |>
  nrow()
```

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  remove_short_term(len = 4) |>
  calculate_doc_freq(kunta, LEMMA) |>
  filter(df > 5, df < 65) |>
  nrow()
```

Very uncommon terms in terms of document frequency: 

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  remove_short_term(len = 4) |>
  calculate_doc_freq(kunta, LEMMA) |>
  filter(df <= 5) |>
  count(LEMMA, sort = TRUE)
```

Very common terms in terms of document frequency: 

```{r}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  remove_short_term(len = 4) |>
  calculate_doc_freq(kunta, LEMMA) |>
  filter(df >= 65) |>
  count(LEMMA, sort = TRUE)
```

Complete preprocessing pipeline: 

```{r, R.options = list(width = 10000, pillar.print_max = 20, pillar.print_min = 20)}
aspol |>
  filter_upostag(c("NOUN", "VERB", "ADJ")) |>
  remove_numbers(LEMMA) |>
  remove_foreign() |>
  remove_short_term(len = 4) |>
  calculate_doc_freq(kunta, LEMMA) |>
  filter(df > 5, df < 65)
```

Complete preprocessing pipeline as a single function: 

```{r, R.options = list(width = 10000, pillar.print_max = 20, pillar.print_min = 20)}
aspol |> preprocess_corpus(doc = kunta, term = LEMMA)
```

