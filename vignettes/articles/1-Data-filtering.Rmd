---
title: "1 Data filtering"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, message=FALSE, warning=FALSE}
library(rfintext)
# library(quanteda)
# library(tidytext)
library(stringr)
library(dplyr)
library(readr)
library(ggplot2)
library(ggwordcloud)
```

Aim is to filter out uninteresting, and potentially false terms. Already filtered
version of `aspol` is saved with name `aspol_filtered`.


From [UPOSTAG](https://turkunlp.org/docs/u/pos/index.html) class we can see
some interesting word classes and less interesting.

```{r}
aspol |> count(LEMMA, UPOSTAG, name = "total", sort = TRUE) |>
  slice_max(order_by = total, n = 1, by = UPOSTAG) |> print(n = 16)
```

* Meaningless: `PUNCT`, `CCONJ`, `AUX`, `SYM`, `PRON`, `ADV`, `SCONJ`, `NUM`, `ADP`, `X`, `_`, `INTJ`, `PROPN`  
* Meaningful: `NOUN`, `VERB`, `ADJ`

```{r}
noun_verb_adj <- aspol |> filter(UPOSTAG %in% c("NOUN", "VERB", "ADJ"))
```

With [FEATS](https://universaldependencies.org/u/feat/index.html) column we can 
filter all abbreviations, foreign terms and ordinal numbers which are of class ADJ even though numbers.

```{r}
noun_verb_adj |> filter(str_detect(FEATS, "Abbr=Yes|NumType=Ord|Foreign=Yes")) |> 
  count(FORM, LEMMA, FEATS, UPOSTAG, sort = TRUE)
```

Possibly interesting is `oy`. Let's keep that and remove rest of abbreviated terms.

```{r}
noun_verb_adj <- noun_verb_adj |> filter(!str_detect(FEATS, "Abbr=Yes|NumType=Ord|Foreign=Yes") | FEATS == "oy")
```

There are many terms which are only one character long in their original form or
in their lemma form.

```{r}
noun_verb_adj |> filter(nchar(FORM) == 1 | nchar(LEMMA) == 1) |> count(FORM, LEMMA, sort = TRUE)
```

Remove all those

```{r}
noun_verb_adj <- noun_verb_adj |> filter(nchar(FORM) > 1, nchar(LEMMA) > 1)
```

Additionally, term m2 gets's lemma for menrer#mammem!?

```{r}
noun_verb_adj |> filter(FORM == "m2") |> count(FORM, LEMMA, sort = TRUE)
```

```{r}
noun_verb_adj <- noun_verb_adj |> filter(FORM != "m2")
```

```{r}
noun_verb_adj
```

Some false terms still. Especially OCR-processed documents like Enonteki√∂ have 
a lot of nonsense terms.

Some terms which appear only in single document:

```{r}
doc_count <- noun_verb_adj |> distinct(LEMMA, kunta) |> count(LEMMA, name = "dc")
noun_verb_adj |> left_join(doc_count) |> filter(dc == 1) |> count(kunta, LEMMA, sort = TRUE)
```

```{r}
noun_verb_adj <-  noun_verb_adj |> left_join(doc_count) |> filter(dc >= 3)  # LEMMA in at least 3 doc
```


```{r, fig.dpi=300, fig.height=5.83, fig.width=8.27}
noun_verb_adj |> count(LEMMA, UPOSTAG) |> slice_max(order_by = n, n = 300) |>
  ggplot(aes(label = LEMMA, size = n, colour = UPOSTAG)) +
  geom_text_wordcloud_area(rm_outside = TRUE, shape = "square", use_richtext = FALSE) +
  scale_size_area(max_size = 30)
```

```{r}
# aspol_filtered <- noun_verb_adj
# usethis::use_data(aspol_filtered, overwrite = TRUE)
```

